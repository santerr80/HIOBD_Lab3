{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1.\tСчитайте файл и создайте Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+---+------+\n",
      "|         feature_1|        feature_2|         feature_3|         feature_4|         feature_5|         feature_6|         feature_7|         feature_8|         feature_9|       feature_10|        feature_11| id|target|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+---+------+\n",
      "|45.531834384241506|46.83396582114066| 91.35670312904635|  45.5062662358562|59.298901864123664|53.208014106424784|40.884734939439575|53.576295742313334| 94.35099858735667|83.27720884117582| 51.97168648767926|  0|     1|\n",
      "| 39.01399279627978|48.17808812469899|56.848193716775945| 82.52671245077899| 63.03232148309602| 69.82781855216082| 90.99202541263514| 47.80387372239317| 93.11273907409596|70.63347744224721| 88.74531021734117|  1|     1|\n",
      "| 38.90616644592203|47.45006399038541| 86.91937085228842| 57.08387025999836| 78.34460989694585| 88.34446684911853| 42.24874011640948| 72.56894315645235|56.492162888754734|68.90553423836026| 45.77511774468021|  2|     1|\n",
      "| 39.13854594340603|89.52023726907673| 48.83311704193416| 81.66270312313289| 62.09754686381496| 42.45953920247878| 78.35016962136672| 54.04583484569447| 56.18899514581124| 86.3655318199856| 62.78989078040118|  3|     1|\n",
      "| 39.49502362137681|57.10891014181148| 41.92094353397613|56.436305291126764| 75.82052615333603| 53.80105569125444|60.963417932825706| 70.50253781839021| 63.74316098980595|73.21140420131302| 71.24360947171856|  4|     1|\n",
      "| 62.04718379593042|86.34857081554053| 48.04383899737042| 63.69727453477824| 72.39940786385952| 90.91467373628377| 90.10519351095468| 75.83888366547019| 94.18119998655747| 88.2435787251389| 78.99336808076602|  5|     0|\n",
      "|  50.9661717129838| 91.2847558640387| 66.53797993759308| 92.97118415753941|   69.615178106179| 71.26603916091477|  47.6814637803475| 71.63903181089096| 58.39205120407184| 79.0351050148708| 89.87015250321147|  6|     1|\n",
      "| 43.37807958794595|82.51253026832391| 85.12349464468515| 68.90710450774208| 82.27851071779565| 41.14327413458715| 90.36982112779057| 89.97461849196733| 75.58090454066168|91.11824601066537| 43.90049285438145|  7|     1|\n",
      "| 80.11883391101706|73.73489567032708|  79.3953814248719|54.076897218929574| 83.56680728666095| 69.41487727680037| 91.90543002545883| 73.46590969046629| 48.53663133754231|48.24159593998617| 78.85902518934009|  8|     1|\n",
      "| 77.07433878768568|61.58489048655755|52.581521454160175| 84.59513896338287|42.392765702605054| 74.26114280083618| 82.11640526083339|42.129095310597094|55.387877917980006|70.46488832719672|  74.1084549304756|  9|     1|\n",
      "| 54.64525690857506|89.58283440563572|55.550428685235694| 82.55344375421295| 54.64378640759897| 61.13938526087483| 64.54000824877426| 68.28155951295258| 77.76713419029697|51.97157318303966|  86.8624984333832| 10|     1|\n",
      "| 82.55628043376608|45.49016107225747| 89.27024710085753| 79.85647920885167| 65.99218137273262| 93.97165295858551| 91.76418122081127| 86.02014417496326| 68.44070979381561|83.37662470882977| 66.28031640613631| 11|     1|\n",
      "| 50.22755116703899|92.00480630177053| 78.04997160653514| 91.82220407199334| 88.31614943171246|60.879635567643696| 52.69742621491881|48.504027532267614|63.663942233401485|53.38283090230161| 71.61713957938723| 12|     0|\n",
      "| 44.13451219797396|78.03295010193851| 68.89066839827096| 60.20659375088306| 49.96094171748632| 57.68271636866895| 39.81614831507314|58.804360129048575| 63.05722110899669|90.52917069133386|  71.5749904367627| 13|     0|\n",
      "| 93.20927747905411|75.93382417181842| 91.39580034613566| 63.72165168411232|  70.7924657195509| 82.72354562149735| 78.22477563054201|  84.7813474711853|49.061037587903435|78.12695019514148| 90.18821638011437| 14|     1|\n",
      "| 67.18929273164865|79.73434601814922| 69.58617744641727|57.395350479829695| 84.96052827554568|51.489258809556745| 55.83209871210015|55.404006005657024|43.412706010203216|41.55683490622736|  93.4869640044059| 15|     1|\n",
      "| 79.70164345961491| 55.0139505276944|52.550907843704046|45.690840765164246| 45.52959458082954| 58.05006528338728| 74.06786996197062| 64.36143483649663| 40.42616736036976|88.22946230833604| 44.94041671273824| 16|     0|\n",
      "|59.981866869569714|51.64374896584491| 51.49417999968945| 78.10302337940982| 83.00117391944354|63.048995253791304|57.302441857963714| 67.44299871573715| 65.91206238956656|51.55391998859438| 74.50452680310384| 17|     1|\n",
      "|61.979306224071316|63.92635409091963| 63.99134527672599| 55.29666729958748| 74.79305322378586| 72.51226191850927| 76.77261072227343|55.404818381773055| 51.31101281323753|56.31879431495884|60.946039834858055| 18|     0|\n",
      "| 48.92681313474963|59.23987294450475|  48.3345126149239|54.421228118329324| 68.18128651626677| 55.40154197767844|  82.4199733628159|46.532717640423186| 80.04047873032003|63.89958701118652|  42.2303724549337| 19|     0|\n",
      "+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV to DataFrame\").getOrCreate()\n",
    "\n",
    "\n",
    "csv_file_path = \"130/1/1.csv\"\n",
    "\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tВыполните преобразование к числовому типу колонки feature_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- feature_1: double (nullable = true)\n",
      " |-- feature_2: double (nullable = true)\n",
      " |-- feature_3: double (nullable = true)\n",
      " |-- feature_4: double (nullable = true)\n",
      " |-- feature_5: double (nullable = true)\n",
      " |-- feature_6: double (nullable = true)\n",
      " |-- feature_7: double (nullable = true)\n",
      " |-- feature_8: double (nullable = true)\n",
      " |-- feature_9: double (nullable = true)\n",
      " |-- feature_10: double (nullable = true)\n",
      " |-- feature_11: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"feature_11\", col(\"feature_11\").cast(\"double\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tВыведите статистические характеристики колонки feature_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        feature_11|\n",
      "+-------+------------------+\n",
      "|  count|               750|\n",
      "|   mean| 66.22745050997271|\n",
      "| stddev| 15.84112966591772|\n",
      "|    min|38.661267623399524|\n",
      "|    max| 93.61276418162505|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stat = df.describe(\"feature_11\")\n",
    "\n",
    "stat = stat.withColumn(\"feature_11\", col(\"feature_11\").cast(\"string\"))\n",
    "\n",
    "stat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tРассчитайте дополнительно коэффициенты асимметрии и эксцесса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "# Коэффициент ассиметрии\n",
    "skew = df.select(skewness(\"feature_11\")).collect()[0][0]\n",
    "\n",
    "# Коэффициент эксцесса\n",
    "kurt = df.select(kurtosis(\"feature_11\")).collect()[0][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  Запишите полученные данные (3,4) в файл JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Создаем новый DataFrame с данными\n",
    "new_data = [{\"skew\": skew, \"kurt\": kurt}]\n",
    "\n",
    "file_path = \"130/1/statistic.json\"\n",
    "\n",
    "# Запись Статистики в JSON файл\n",
    "stat.toPandas().to_json(file_path, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "with open(file_path, 'a+') as json_file:\n",
    "    json.dump(new_data, json_file, indent=4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
